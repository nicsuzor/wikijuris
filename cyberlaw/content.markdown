---
layout: default
title: Content Regulation
parent: Governing the Internet
nav_order: 90
---
[Edit this page](https://github.com/nicsuzor/wikijuris/blob/master/cyberlaw/content.markdown){: .btn .btn-outline }


# Content Regulation and Online Classification
{: .no_toc }

1. Table of Contents
{:toc}


Australia has a co-regulatory content regulation scheme. Under a co-regulation model, an industry body (such as the Communications Alliance) usually develops a code of practice, which is then made binding on industry participants through a legislative mechanism. Co-regulation is a common form of regulation in Australian media law.

The _Online Safety Act 2021_ (Cth) sets out an expectation that industry bodies or associations will develop industry codes to regulate certain types of harmful online material. The Act provides for the eSafety Commissioner to register the codes if certain conditions are met. These include, among other things, that the Commissioner was consulted on the code and the Commissioner is satisfied that:

•	The code was developed by a body or association that represents a particular section of the online industry, and the code deals with one or more matters relating to the online activities of those participants.

•	To the extent to which the code deals with one or more matters of substantial relevance to the community—the code provides appropriate community safeguards for that matter or those matters.

•	To the extent to which the code deals with one or more matters that are not of substantial relevance to the community—the code deals with that matter or those matters in an appropriate manner.

•	The body or association published a draft of the code and invited members of the public and industry participations to make submissions, and gave consideration to any submissions that were received.

The Commissioner may also request that a particular body or association which represents a section of the online industry develop an industry code dealing with one or more specified matters relating to the online activities of those industry participants. In April 2022, the Commissioner issued such a request, seeking the development of codes relating to ‘class 1’ material by six industry associations. The associations submitted draft codes in November 2022.

Watch the following videos for background on online content regulation prior to the 2021 changes:

* **Video Overview of Online Content Regulation in Australia by [Nicolas Suzor](https://www.youtube.com/watch?v=MXx8E2jnyGg)**

* [Code for Industry Co-Regulation in Areas of Mobile and Internet Content](http://www.acma.gov.au/~/media/Content%20Classification/Regulation/pdf/Internet%20Industry%20Codes%20of%20Practice%202005.PDF) (2005)

## Content classification in Australia

**Video Overview of Australia's classification Ratings by [Emily Rees](https://www.youtube.com/watch?v=GAZ3Bev5lb0)**

The rules that apply to content depend upon the classification of the content. Australia has a national classification scheme for content (films, games, publications) likely to cause offence which was enacted in 1995 – the National Classification Scheme/Code.
The _Online Safety Act_ establishes an online content scheme which is partly dependent upon classification under the National Classification Code. As such, an overview of the basic features of the code supports an understanding of the _Online Safety Act_ scheme. 

The National Classification Code provides a statement of purpose that classification decisions are to give effect, as far as possible, to the following principles:
(a)	adults should be able to read, hear and see what they want;
(b)	minors should be protected from material likely to harm or disturb them;
(c)	everyone should be protected from exposure to unsolicited material that they find offensive;
(d)	the need to take account of community concerns about:
(i)	depictions that condone or incite violence, particularly sexual violence; and
(ii)	the portrayal of persons in a demeaning manner.

Publications, Films, and Computer games are rated by the Classification Board, according to the [Classification Guidelines](http://www.comlaw.gov.au/Series/F2005L01286). Each State and Territory determines the consequences of classification. The ratings systems differ by media type:

* Films: G, PG, M, MA15+, R18+, X18+, RC
* Publications: Unrestricted, Unrestricted (M), Category 1 Restricted, Category 2 Restricted, RC
* Games: G, PG, M, MA15+, R18+, RC


### Classification Guidelines: R18+ ###

* High impact violence, simulated sex, drug use, nudity
* No restrictions on  language

### Classification Guidelines: X18+ ###

* Real depictions of sexual intercourse and sexual activity between consenting adults
* No depiction of violence or sexual violence
* No sexually assaultive language
* No consensual activities that ‘demean’ one of the participants
* No fetishes (such as ‘body piercing’; candle wax; bondage; fisting; etc)
* No depictions of anyone under 18, or of adults who look under 18.

### Classification Guidelines: Refused Classification (RC) ###
“Publications that  appear to  purposefully  debase or abuse  for the enjoyment  of readers/viewers,  and which lack  moral, artistic or  other values to the  extent that they  offend against  generally accepted  standards of  morality, decency  and propriety will  be classified ‘RC’.”

For films, anything that exceeds X18+ is Refused Classification. For Games, anything that exceeds R18+ is RC (A new R18+ category was introduced for Games in 2012).

Classification Guidelines: RC (Films)

* Detailed instruction in crime or violence
* Descriptions or depictions of child sexual abuse or any other exploitative or offensive descriptions or depictions involving a person who is, or appears to be, a child under 18 years.
* Violence: Gratuitous, exploitative or offensive depictions of:
  * violence with a very high degree of impact or which are excessively frequent, prolonged or detailed;
  * cruelty or real violence which are very detailed or which have a high impact;
  * sexual violence.

* Sexual activity: “Gratuitous, exploitative or offensive depictions of:
  * activity accompanied by fetishes or practices which are offensive or abhorrent;
  * incest fantasies or other fantasies which are offensive or abhorrent.”

* Drug use:
  * Detailed instruction in the use of proscribed drugs.
  * Material promoting or encouraging proscribed drug use.



## Online Content Scheme

The online content scheme under the _Online Safety Act_ relates to two kinds of material: ‘class 1 material’ and ‘class 2 material’. Pursuant to s 106, class 1 material is material which is, or would likely be, classified as ‘RC’ by the Classification Board. Pursuant to s 107, class 2 material is material which is, or would likely be, classified as X 18+, R 18+, Category 2 or Category 1 restricted. 

The Act provides for the notice and removal of class 1 material. Sections 109 and 110 provides that the Commissioner may give a notice to certain online service providers (including social media and hosting services) to remove or cease hosting material which the Commissioner is satisfied is class 1 material that can be accessed by end-users in Australia. It is not relevant where the service is provided from, or where the material is hosted – it merely needs to be accessible from Australia.

The notice may require the service provider to take all reasonable steps to remove the material from the service within 24 hours or such longer period specified by the Commissioner. Section 111 requires the service provider to comply with a removal notice to the extent they are capable of doing so. 

The Act also provides for the notice and removal of certain class 2 material, namely material classified or likely classifiable as X 18+ or Category 2 restricted. The Commissioner may issue a notice to the relevant provider under ss 114 or 115. In this case, the location of the services or hosting is relevant. The Commissioner may only issue notices in relation to services provided from Australia, or content hosted within Australia. Pursuant to s 116, the provider must comply with the notice to the extent capable of doing so. 

With respect to class 2 material which falls within the R 18+ or category 1 restricted classifications, the Commissioner has the power to give the provider a remedial notice under s 119. The notice may require the relevant provider to remove the material or ensure that the material is subject to a ‘restricted access system’. A restricted access system is an access-control system which the Commissioner declares to be a ‘restricted access system’. In essence, these are systems which limit the exposure of person under 18 to ‘age-inappropriate’ content online. 

Under s 124, the Commissioner also has the power to issue notice to search engine providers requiring the provider to cease providing links to class 1 materials (a ‘link deletion notice’) in certain circumstances. Under s 128, the Commissioner may issue notice to an app distribution service provider to cease enabling end users in Australia to download an app that facilitates the posting of class 1 material (an ‘app removal notice’) in certain circumstances.

## Basic Online Safety Expectations

The _Online Safety Act_ provides for to Minister for Communications to make a determination (a form of legislative instrument) setting out basic online safety expectations.

The first determination was made in 2022. The _Online Safety (Basic Online Safety Expectations) Determination 2022_ specifies the basic online safety expectations for a social media service and other services that allow end users to access material using a carriage service or a service that delivers material by means of a carriage service.

Under s 49, the Commissioner may require the relevant providers to submit periodic reports on how they are meeting the expectations set out in the determination. The Commissioner may also publish statements about the provider’s compliance or non-compliance with the expectations on its website.

# Section 313 of the Telecommunications Act 1997 (Cth)


**Video Overview by Kaava Watson:[Section 313](https://www.youtube.com/watch?v=DgSMz2GRVB4)**

In Australia, several different forms of pressure have been exercised in recent years to encourage intermediaries to take action to police the actions of their users. The most blunt is direct action by law enforcement agencies, who are empowered to make requests of telecommunications providers under s 313 of the Telecommunications Act.  This provision requires carriers and carriage service providers to "do the carrier's best or the provider's best to prevent telecommunications networks and facilities from being used in, or in relation to, the commission of offences against the laws of the Commonwealth or of the States and Territories", and to
 “give officers and authorities of the Commonwealth and of the States and Territories such help as is reasonably necessary” to enforce criminal law, impose pecuniary penalties, assist foreign law enforcement, protect the public revenue, and safeguard national security.


 **Gab Red Explains [How s 313 Is Used by Government Agencies to Block Websites](https://www.youtube.com/watch?v=tdfHpMizgkM)**

 **Matt Cartwright [Explains](https://www.youtube.com/watch?v=CDxI6-ePEgk) the Recommendations of the Recent [Inquiry Into the Use of s 313](http://www.aph.gov.au/Parliamentary_Business/Committees/House/Infrastructure_and_Communications/Inquiry_into_the_use_of_section_313_of_the_Telecommunications_Act_to_disrupt_the_operation_of_illegal_online_services)**

The section essentially enables police and other law enforcement agencies to direct ISPs to hand over information about users and their communications. Increasingly, however, it is also apparently used by a number of government actors to require service providers to block access to content that appears to be unlawful, in cases ranging from the Australian Federal Police seeking to block access to child sexual abuse material to the Australian Securities and Investment Commission (ASIC) blocking access to phishing websites. Even the RSPCA is reported to have used the power, although the details of its request are not clear. There is significant concern over the lack of transparency around s 313(3) and lack of safeguards over its use.[^AUTOREPLACEDSeeforexampleAlanaMaurushatDavidVaileandAliceChowTheAftermathofMandatoryInternetFilteringandS313oftheTelecommunicationsAct1997Cth201419MediaandArtsLawReview263AUTOREPLACED] These came to the fore in 2013 when ASIC asked an ISP to block a particular IP address, not realising that the address was shared between up to 250,000 different websites, including the Melbourne Free University. 

[^AUTOREPLACEDSeeforexampleAlanaMaurushatDavidVaileandAliceChowTheAftermathofMandatoryInternetFilteringandS313oftheTelecommunicationsAct1997Cth201419MediaandArtsLawReview263AUTOREPLACED]: See, for example, Alana Maurushat, David Vaile and Alice Chow, ‘The Aftermath of Mandatory Internet Filtering and S 313 of the Telecommunications Act 1997 (Cth)’ (2014) 19 Media and Arts Law Review 263.



# Image-based abuse

**Video overview of [image-based abuse laws](https://www.youtube.com/watch?v=IW17HxwdP-Q) by Danielle Harris**

The non-consensual sharing of intimate images is often colloquially referred to as 'revenge porn'. The term 'image-based abuse' is generally considered to be a better term because it avoids the victim-blaming connotations that the abuse is done in 'revenge' for some perceived wrong.

The National Statement of Principles Relating to the Criminalisation of the Non-consensual Sharing of Intimate Images encouraged each Australian jurisdiction to adopt nationally consistent criminal offences.

Under the _Criminal Code Act 1995_ (Cth), it is an offence to post, or threaten to post, non-consensual intimate images.[^AUTOREPLACEDEnhancingOnlineSafetyNonConsensualSharingofIntimateImagesAct2018Cthsch2s4CriminalCodeAct1995Cths47417AAUTOREPLACED] 
Specifically, s 474.17 of the Criminal Code sets out an offence for the use of a carriage service in a way that reasonable persons would regard as being, in all the circumstances, menacing, harassing or offensive. Section 474.17A makes it an aggravated offence where that use involves transmitting or promoting material that is private sexual material.

Section 75 of the _Online Safety Act_ prohibits the posting, or threatened posting, of an intimate image of another person without their consent. The prohibition applies where the person in the image or person posting the image are ordinarily resident in Australia. An ‘intimate image’ is defined to include images that depict genital or anal areas, a female, transgender or intersex person’s breasts, private activities such as showering, using the toiler or engaging in a sexual act not ordinarily done in public.

There is also a complaints-based system in the _Online Safety Act_, whereby the eSafety Commissioner may issue a removal notice or another civil remedy upon receipt of a victim’s complaint.


[^AUTOREPLACEDEnhancingOnlineSafetyNonConsensualSharingofIntimateImagesAct2018Cthsch2s4CriminalCodeAct1995Cths47417AAUTOREPLACED]: Enhancing Online Safety (Non-Consensual Sharing of Intimate Images) Act 2018 (Cth) sch 2 s 4; Criminal Code Act 1995 (Cth) s 474.17A.

[^AUTOREPLACEDEnhancingOnlineSafetyNonConsensualSharingofIntimateImagesAct2018Cthsch1s24EnhancingOnlineSafetyAct2015Cthss19A2744D44FAUTOREPLACED]: Enhancing Online Safety (Non-Consensual Sharing of Intimate Images) Act 2018 (Cth) sch 1 s 24; Enhancing Online Safety Act 2015 (Cth) ss 19A, 27, 44D–44F.



Queensland extended the definition of ‘intimate’ images to include original or photoshopped still or moving images of a person engaged in intimate sexual activity; a person's bare genital or anal region; or a female, transgender or intersex person's breasts.[^AUTOREPLACEDCriminalCodeNonConsensualSharingofIntimateImagesAmendmentBill2018Qlds4CriminalCodeAct1899Qlds207AAUTOREPLACED]

The definition covers an image that has been altered to appear to show any of the above-mentioned things.

[^AUTOREPLACEDCriminalCodeNonConsensualSharingofIntimateImagesAmendmentBill2018Qlds4CriminalCodeAct1899Qlds207AAUTOREPLACED]: Criminal Code (Non-Consensual Sharing of Intimate Images) Amendment Bill 2018 (Qld) s 4; Criminal Code Act 1899 (Qld) s 207A.


The State also introduced three new misdemeanours into their Criminal Code to broaden the scope of conduct which is captured under the offence. These include distributing intimate images without the consent of the person depicted,[^AUTOREPLACEDCriminalCodeNonConsensualSharingofIntimateImagesAmendmentBill2018Qlds5CriminalCodeAct1899Qlds223AUTOREPLACED] observing or recording breaches of privacy,[^AUTOREPLACEDCriminalCodeNonConsensualSharingofIntimateImagesAmendmentBill2018Qlds6CriminalCodeAct1899Qlds227AAUTOREPLACED] and distributing prohibited visual recordings.[^AUTOREPLACEDCriminalCodeNonConsensualSharingofIntimateImagesAmendmentBill2018Qlds7CriminalCodeAct1899Qlds227BAUTOREPLACED]


[^AUTOREPLACEDCriminalCodeNonConsensualSharingofIntimateImagesAmendmentBill2018Qlds5CriminalCodeAct1899Qlds223AUTOREPLACED]: Criminal Code (Non-Consensual Sharing of Intimate Images) Amendment Bill 2018 (Qld) s 5; Criminal Code Act 1899 (Qld) s 223.

[^AUTOREPLACEDCriminalCodeNonConsensualSharingofIntimateImagesAmendmentBill2018Qlds6CriminalCodeAct1899Qlds227AAUTOREPLACED]: Criminal Code (Non-Consensual Sharing of Intimate Images) Amendment Bill 2018 (Qld) s 6; Criminal Code Act 1899 (Qld) s 227A.


[^AUTOREPLACEDCriminalCodeNonConsensualSharingofIntimateImagesAmendmentBill2018Qlds7CriminalCodeAct1899Qlds227BAUTOREPLACED]: Criminal Code (Non-Consensual Sharing of Intimate Images) Amendment Bill 2018 (Qld) s 7; Criminal Code Act 1899 (Qld) s 227B.



## Social context and prevalence

### Key statistics

The prevalence of image-based abuse was highlighted in a study conducted by the eSafety Commissioner in 2017 that found that 1 in 10 individuals experienced image-based abuse, with females aged between 15 to 17 years being most at risk. The report also found:

- 6 in 10 victims knew the perpetrator;
- The perpetrator was a friend that they knew offline (29%), an ex-partner (13%), a current partner (12%) or a family member (10%); and
- Image-based abuse is more likely to occur on Facebook (53%).

### Factors affecting reporting

The eSafety Commissioner is empowered to investigate and make decisions regarding image-based abuse but this requires victims to report it. Studies have estimated that only 35% of cases of image-based abuse are reported. The factors influencing a victim not reporting can include:

- Negative stigma;
- Psychological barriers including victim blaming, humiliation and embarrassment;
- An unawareness of the severity of the incident;
- A fear of exacerbating or making it worse including attention to image-based abuse;
- Lack of confidence in law enforcement; and
- Unaware of the support services available.

### Social prevention and response

Considering the social context of image-based abuse, there have been several actions taken by the Australian Government and other bodies to raise awareness and better educate individuals:

1. The Office of the eSafety Commissioner has developed a professional learning program for teachers and facilitators titled 'Online Harmful Sexual Behaviours, Misinformation and Emerging Technology'. Its goal is to equip individuals with the necessary skills to identify and respond to incidences of image-based abuse and the role that coercion plays.

2. In 2020, the NSW Government launched a campaign to help prevent image-based abuse and educate individuals on the topic including information on where to seek help. This was largely due to the number of reports between 2019 to 2020, namely a 172% increase. The campaign also offered counselling to individuals and the removal of the content.

3. In 2022, the Australian Government responded to the report from the Senate titled 'Phenomenon colloquially referred to as 'revenge porn''. The statement supported most of the recommendations proposed including that all police officers undertake mandatory training around image-based abuse.

**Angelina Kardum explains: [How the major social media platforms deal with image-based abuse](https://www.youtube.com/watch?v=Y-RbaE73B7M&feature=youtu.be)**

Most major social media sites now have policies against image-based abuse in their community guidelines or standards. Victims of image-based abuse can make a report directly to the site on which their intimate image was shared. This report is then assessed against the site's community guidelines or standards and if it appears to be in violation of the community guidelines or standards, the image is generally removed within 24 hours. Whilst major social media services have taken positive steps towards tackling image-based abuse, such as developing reporting and take-down mechanisms, these mechanisms have their shortcomings. The two main issues with the current approaches taken by social media services are the delays associated with the assessment of reports and the heavy reliance on self-reporting.

The responses from online service providers to the issue of image-based abuse include:

1. In February 2015, Reddit updated its private policy to prohibit the publication of image-based abuse;
2. In March 2015, Twitter (now known as 'X') announced that they would immediately remove any link image-based abuse upon request; and
3. In June/July 2015, Google and Microsoft announced they would remove links upon request.

# The Office of the e-Safety Commissioner

**Lauren Trickey explains [how to make a complaint to the eSafety Commissioner](https://youtu.be/2aTxXyZsYgw)**

The [eSafety Commissioner](https://www.esafety.gov.au/) is a statutory office which was first established by the Enhancing Online Safety Act 2015 (Cth) to promote and enhance online safety. The powers of the Commissioner were later enhanced in the _Online Safety Act 2021_ (Cth). While most of the Commissioner’s functions are contained in the Online Safety Act 2021, the Commissioner also has powers and functions under the Telecommunications Act 1997 (Cth) and the Criminal Code Act 1995 (Cth).

The Commissioner can receive reports for cyber-bulling, image-based abuse or offensive and illegal content.

Under s 30, complaints about cyberbullying of a child can be made by an Australian child or parent, guardian or person authorised by the child. An adult person can also make a complaint if they believe they were the target of cyberbullying material as a child, so long as the complaint is made within a reasonable time after they became aware and 6 months after they reached 18 years old. Cyberbullying refers to online material intended to seriously threaten, intimidate, harass or humiliate an Australian child. 

The 2021 amendments introduced the world's first legal scheme dealing with cyberbullying of adults. Under s 36, an Australian adult may make a complaint to the Commissioner about cyber-abuse material. Cyber-abuse material is material an ordinary reasonable person would conclude is likely intended to have an effect of causing serious harm to a particular Australian adult; and an ordinary reasonable person in the position of the Australian adult would regard the material as being, in all the circumstances, menacing, harassing or offensive.

As outlined above, image based-abuse complaints can be made to the Commissioner. Pursuant to s 32, complaints can be made by the person in the intimate image, a person authorised to make a report or a parent or guardian of a child or a person who does not have capacity. 

Australian residents can also report offensive or illegal content, which includes abhorrent violent material or material depicting illegal acts.

For each type of material an online form can be completed on the eSafety website. Each form requests information regarding what is contained or depicted in the material and where the material has been posted. After receiving a complaint, , the Commissioner has the power to conduct an investigation (as the Commissioner thinks fit). The Commissioner assesses the material complained of to determine the appropriate course of action, which may include liaising with the relevant platform for the material to be removed.

# Abhorrent Violent Material

**See overview by Georgie Vine about the [Criminal Code Amendment (Sharing of Abhorrent Violent Material) Act 2019](https://www.youtube.com/watch?v=a8qDarI5mCM)**

The _[Criminal Code Amendment (Sharing of Abhorrent Violent Material) Act 2019](https://www.legislation.gov.au/Details/C2019A00038)_ creates new offences under the _Criminal Code Act 1995_ (Cth), effective from 6 April 2019. 

These new provisions require content (social media) websites and hosting providers to remove abhorrent violent material as soon as reasonably possible, and to refer it to the Australian Federal Police. 

These offences target content that is reasonably capable of being accessed with Australia, regardless of where the material was created or where the platform operator is located. The offences include substantial penalties if individuals and companies do not remove or report such classified material: fines up to 10% of annual global turnover for companies, and up to 3 years imprisonment for individuals.

There are a few defences to the new offences, including material necessary for law enforcement, material distributed by journalists; material used for scientific, medical, academic or historical research; and the exhibition of artistic works.

# Regulating content in other jurisdictions

**Snoot Boot explains [France and Germany's online hate speech laws](https://www.youtube.com/watch?v=OxLjyjaFiiY)**

Both France and Germany have attempted stricter approaches to regulating online hate speech. In particular, Germany's laws require social media companies to remove hate speech and report users to the police, or else face significant fines. In 2020, France proposed laws similar to Germany's, but these laws were struck down by a French court, as the laws were unconstitutional and imposed an unreasonable burden on the freedom of speech because they incentivized over-censorship. The laws highlight a deeper tension between free speech and the need to regulate and censor hateful ideologies being spread online.

### United Kingdom

The United Kingdom's Online Safety Act 2023 represents a comprehensive approach to platform regulation that extends beyond Australia's co-regulatory model. The Act imposes direct duties on social media platforms and search engines to protect users from harmful content. The Office of Communications (Ofcom) is implementing the Act through codes of practice addressing illegal content, content harmful to children, and specific categorised services.[^UKOnlineSafetyAct]

The UK Act introduces several new criminal offences that address gaps in existing law:

- **Intimate image abuse** (s 188) - criminalising the non-consensual sharing of intimate images, similar to Australia's image-based abuse provisions
- **Epilepsy trolling** (s 183) - targeting those who send content designed to trigger seizures
- **False communications** (s 179) - prohibiting the sending of false information intended to cause non-trivial harm
- **Threatening communications** (s 181) - modernising threats law for digital contexts
- **Encouraging self-harm** (s 184) - addressing content that encourages or assists serious self-harm
- **Cyberflashing** (s 187) - criminalising the unsolicited sending of sexual images

These offences demonstrate how jurisdictions are identifying and addressing specific online harms through targeted criminal law provisions, complementing broader platform regulation approaches.

### Canada

Canada's proposed Online Harms Bill 2024 takes a duty-based approach to platform regulation.[^CanadaOnlineHarmsBill] The Bill would impose three primary duties on social media services:

1. **Duty to act responsibly** - requiring platforms to implement systems to address harmful content
2. **Duty to protect children** - specific obligations regarding content accessible to minors
3. **Duty to make certain content inaccessible**, specifically:
   - Content that sexually victimises a child or revictimises a survivor
   - Intimate images posted without consent

This approach emphasises proactive obligations on platforms rather than reactive content removal, reflecting an emerging trend in online safety regulation.[^CanadaOnlineHarmsInfo]

[^UKOnlineSafetyAct]: United Kingdom, Department for Science, Innovation & Technology 'Guidance -- Online Safety Act: explainer' *Online Safety Act: explainer* (Web Page, 8 May 2024) <https://www.gov.uk/government/publications/online-safety-act-explainer/online-safety-act-explainer#what-the-online-safety-act-does>.

[^CanadaOnlineHarmsBill]: Government of Canada, 'Government of Canada introduces legislation to combat harmful content online, including the sexual exploitation of children' *Canadian Heritage* (Web Page, 26 February 2024) <https://www.canada.ca/en/canadian-heritage/news/2024/02/government-of-canada-introduces-legislation-to-combat-harmful-content-online-including-the-sexual-exploitation-of-children.html>.

[^CanadaOnlineHarmsInfo]: Government of Canada, 'Proposed Bill to address Online Harms' *Arts and media* (Web Page, 4 April 2024) <https://www.canada.ca/en/canadian-heritage/services/online-harms.html>.


# Other emerging issues

## Deepfakes

**See [an explanation of deepfakes](https://youtu.be/cLHdOJr7v5w) by Eric Briese**

### Deepfakes and Non-consensual Sexual Imagery

A deepfake is a technique of image manipulation where artificial intelligence and deep learning is leveraged to manipulate a person's characteristics, including physical appearance and voice, to create images or videos that appear authentic. While manipulated media is not new, deepfake technology has lowered the technical barriers to creating convincing fabrications, raising significant legal and ethical concerns.

The primary legal concerns with deepfakes relate to their use in creating non-consensual sexual imagery, political disinformation, fraud, and harassment. This section focuses on the legal frameworks addressing non-consensual sexual deepfakes, which constitute a form of image-based sexual abuse.

#### Australian Legal Framework

Australia lacks comprehensive deepfake-specific legislation, but several existing laws may apply depending on the context:

- *Criminal Code Act 1995* (Cth);
- *Telecommunications Act 1997* (Cth);
- *Enhancing Online Safety (Non-consensual Sharing of Intimate Images) Act 2018* (Cth); and
- *Online Safety Act 2021* (Cth).

Most Australian jurisdictions have criminal offences covering non-consensual sharing of intimate images, with varying application to altered material. Federal offences under sections 474.17 and 474.17A of the *Criminal Code Act 1995* (Cth) prohibit using carriage services to menace, harass or offend, including through sharing intimate images. Victoria leads in explicit deepfake criminalisation, with section 53 of the *Crimes Act 1958* (Vic) specifically addressing both production and distribution of deepfake intimate images.

The *Online Safety Act 2021* (Cth) empowers the eSafety Commissioner to issue removal notices to online service providers hosting intimate imagery, including deepfakes. Providers must remove content within 24 hours of notice, with penalties for non-compliance. As noted by scholars, 'detection without removal offers little solace to those exploited by deepfake pornography'.[^DeepfakeTong]

The limitations of civil enforcement mechanisms are illustrated by *Anthony Rondondo v eSafety Commissioner* (2023), where contempt proceedings were required after non-compliance with a removal notice. Rondondo was ordered to pay $25,000 plus costs.[^DeepfakeRondondo] This apparently did not serve as a deterrent; Rondondo was subsequently arrested for distributing deepfake images of school students and teachers.

#### Criminal Code Amendment (Deepfake Sexual Material) Bill 2024

The *Criminal Code Amendment (Deepfake Sexual Material) Bill 2024* represents Australia's first targeted legislative response to deepfake sexual abuse. The Bill introduces specific offences for:

- Creating deepfake sexually explicit content without consent
- Distributing such material (maximum 6 years imprisonment)
- Aggravated offences for creators who also distribute (maximum 7 years)
- Repeat offending (maximum 7 years)

Critics argue the Bill duplicates existing offences and may impact freedom of expression, though supporters emphasise the need for specific deterrence given the unique harms of deepfake technology.[^DeepfakeCriticism]

#### International Approaches

##### United Kingdom

In April 2024, the UK government announced plans to amend the *Criminal Justice Bill* to include a new offence for making sexually explicit deepfakes without consent, which will build on section 66B of the *Sexual Offences Act 2003* (UK).

##### United States

The United States lacks comprehensive federal deepfake legislation, with regulation occurring primarily at state level. California and Texas pioneered state-level deepfake laws in 2019:

California's [Assembly Bill 602](https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201920200AB602) establishes civil remedies for victims of non-consensual pornographic deepfakes, while [Assembly Bill 730](https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201920200AB730) prohibits distribution of political deepfakes within 60 days of elections. Texas similarly prohibits political deepfakes within 30 days of elections.

At the federal level, proposed legislation includes:
- The [DEEP FAKES Accountability Act](https://www.congress.gov/bill/118th-congress/house-bill/5586/text) - establishing criminal penalties and a Department of Homeland Security task force
- The [DEFIANCE Act of 2024](https://www.congress.gov/bill/118th-congress/house-bill/7569/text) - providing civil remedies for victims of sexually explicit deepfakes

##### European Union

The [EU Directive on combating violence against women and domestic violence](https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=OJ:L_202401385&qid=1716884102079) (May 2024) requires member states to criminalise non-consensual production and distribution of sexual deepfakes using artificial intelligence.

##### China

China has taken a comprehensive regulatory approach since 2019. The 'Regulations on the Administration of Networked Audiovisual Information Services' require disclosure when deepfake technology is used and prohibit unlabelled deepfake content.[^DeepfakeChina] The 2023 'Regulations on Deep Synthesis Management of Internet Information Service' extend controls throughout the deepfake lifecycle, requiring platforms to obtain consent before using individuals' likenesses and strengthen training data management.

[^DeepfakeTong]: Tong, S, "You Won't Believe What She Does!': an Examination into the Use of Pornographic Deepfakes as a Method of Sexual Abuse and the Legal Protections Available to its Victims" [2022] *UNSWLawJlStuS* *25; UNSWLJ Student Series* No 22-25.

[^DeepfakeRondondo]: See Laura Lavelle, 'Antonio Rotondo guilty of contempt of court after allegedly creating deepfake images of school students and teachers' (ABC News) (6 December 2023) <https://www.abc.net.au/news/2023-12-06/qld-deepfake-images-court-charge-antonio-rotondo-school-students/103195578>

[^DeepfakeCriticism]: Billi Fitzsimmons, 'A Victorian teen has been arrested after fake nudes of 50 school girls were shared online' *The Daily Aus* (online, 13 June 2024) < https://www.newsletter.thedailyaus.com.au/p/teen-arrested-fake-ai-images>.

[^DeepfakeChina]: Cyberspace Administration of China, Regulations on the Administration of Networked Audiovisual Information Services (18 November 2019) http://www.cac.gov.cn/2019-11/29/c_1576561820967678.htm [perma.cc/E2DQ-ZHCQ].
